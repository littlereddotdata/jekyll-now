# Easing into Month 2 

Month 1 focused on the art of data analysis. We talked about tips for presenting an analysis effectively, the pain points in the data science process, how to use charts and graphs to build an intuition of our data and our models. While reproducibility was a theme that ran through the first month, for the Month 2 we wanted to revisit this concept. this time, we approach reproducibility from a data engineering point of view. By working with practices taken f

# Make theory and practice work together 

Get exposure to theory from Monday walkthroughs, Weekly Readings and Friday wrap-ups. Apply knowledge to industry projects. Issues that pop up during the project are a chance to revisit theory covered earlier to deepen knowledge. 

# Reproducible Data Project Structure 
Each project also comes with metadata (or overhead, if you will) that needs to be tracked to fully understand how the analysis was produced. 

- why did we omit these samples? 
- scripts history and data provenance should be tracked 

This ties in with other conversations around how to communicate the context around an analysis instead of back-engineering a developer's thought-processes through only a paper's graphs, charts and other artefacts. 

[Whole Tale](https://dashboard.wholetale.org/login). 
[Jupyter Notebooks](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks). 
[some notes on context compatibility](https://simplystatistics.org/2018/05/24/context-compatibility-in-data-analysis/). 

After walking through the whys of reproducibility, our mentors demonstrated how to turn an untidy project into something more reproducible. 

# Better Python Programming for Data Scientists
this borrows from the [carpentries.org](https://carpentries.org) that teaches better computing skills to researchers who improve their day-to-day workflows. There's also quite a lot written about helping R programmers, who usually come from a researcher background, implement best practices from software engineering into their work. 

## How to share common tools across Python environments 
move commonly-used functions with a `utils.py` script, and put multiple scripts into a folder that can be called as a module with `__init__.py`
 
`direnv` for managing secrets and keys

# Reading homework
[Hitchhiker's Guide to Python](http://docs.python-guide.org/en/latest/)
I'd like to focus on the sections Writing Great Python Code and the Scenario Guide on Scientific Applications
https://azure.microsoft.com/en-us/blog/how-i-choose-which-services-to-use-in-azure/

# Thoughts
The Python Ecosystem is rich. I've come to understand that this means we can get to 80% on most problems just by surveying what's available within the ecosystem. For example, Katherine Jarmul's jumpstart on data pipelining is great mostly because, imho, she does such a comprehensive job of surveying the useful tools and options Python offers.  https://www.youtube.com/watch?v=b2kB_FuAapI. 
